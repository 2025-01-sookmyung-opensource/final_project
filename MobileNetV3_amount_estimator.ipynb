{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHY7f0YczOCOYHSdPq13rw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2025-01-sookmyung-opensource/final_project/blob/pre-min/MobileNetV3_amount_estimator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kMrNUWqHb8v",
        "outputId": "e6eb3d35-7fce-45aa-e711-ec99fba0b171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "#mobilenet 사전학습 모델 불러오기\n",
        "mobilenet = models.mobilenet_v3_small(pretrained=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od56yfZjeiIs",
        "outputId": "7300647e-11f3-416e-8bf8-3ea28b463ba7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 데이터 경로\n",
        "data_dir = \"/content/drive/MyDrive/image-20250617T154733Z-1-001/image\"\n",
        "q_labels = ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
        "\n",
        "# 잘 올라갔는지 확인\n",
        "print(\"클래스 디렉토리들:\", os.listdir(data_dir))\n"
      ],
      "metadata": {
        "id": "9IOdQmKcj0Pe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05104d13-1f07-460a-dc65-a9e6d2155464"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "클래스 디렉토리들: ['꽁치조림', '닭볶음탕', '닭죽', '깻잎장아찌', '녹두빈대떡', '꽃게탕', '달걀말이', '닭튀김', '닭갈비', '김치볶음밥', '김밥', '곱창전골', '감자탕', '갈비탕', '감자튀김(스틱형)', '감자튀김(웨지감자)', '갈치조림']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class QuantityDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        #self.image_paths = []\n",
        "        #self.labels = []\n",
        "        self.transform = transform\n",
        "        self.root_dir = root_dir\n",
        "        self.q_levels = ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
        "        #self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "        self.samples = []\n",
        "\n",
        "        for food_name in os.listdir(root_dir):\n",
        "            food_path = os.path.join(root_dir, food_name)\n",
        "            if not os.path.isdir(food_path):\n",
        "                continue\n",
        "\n",
        "            for q_idx, q in enumerate(self.q_levels):\n",
        "                q_path = os.path.join(food_path, q)\n",
        "                if not os.path.isdir(q_path):\n",
        "                    continue\n",
        "\n",
        "                for fname in os.listdir(q_path):\n",
        "                    if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        img_path = os.path.join(q_path, fname)\n",
        "                        self.samples.append((img_path, q_idx))  # Q1=0 ~ Q5=4\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "HdUWRqZnzSgE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "# 이미지 전처리\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),   # MobileNetV3 input size\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# 데이터셋 불러오기\n",
        "dataset = QuantityDataset(root_dir=data_dir, transform=transform)\n",
        "\n",
        "# 데이터 분리 (예: 80% train, 20% val)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "# -> generator 추가.  항상 동일하게 나누고 싶어서 선택.\n",
        "\n",
        "# DataLoader 정의\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "n7bO0tvIuhSZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "#사전학습된 모델 불러오기\n",
        "model = models.mobilenet_v3_small(pretrained=True)\n",
        "#마지막 레이어를 클래스 수에 맞게 바꾸는 코드.\n",
        "model.classifier[3] = nn.Linear(model.classifier[3].in_features, 5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7r6egJfu1nC",
        "outputId": "8ea729da-ff0b-4ab1-971a-40a81cab3021"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 장치 설정 - GPU 사용 / 모델 학습 성능에 영향\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#GPU로 모델을 옮겨줌.\n",
        "model = model.to(device)\n",
        "\n",
        "# 손실 함수와 최적화기- 클래스 분류\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) #모델의 가중치 업데이트\n"
      ],
      "metadata": {
        "id": "GIcHdZmZvh8v"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2  # 원하는 만큼 조정\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "   #training 학습\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # 통계\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    #validation 검증\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}] Loss: {running_loss:.3f} | Train Acc: {acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_LzgwONwy-n",
        "outputId": "ac49f817-044d-4dd8-fa79-c086f25c0a1d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Loss: 36.971 | Train Acc: 31.28% | Val Acc: 21.39%\n",
            "[Epoch 2] Loss: 23.961 | Train Acc: 56.42% | Val Acc: 21.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 테스트 이미지 경로\n",
        "img_path = \"/content/drive/MyDrive/김밥_Q5.jpg\"\n",
        "\n",
        "# 이미지 열기 + 전처리\n",
        "image = Image.open(img_path).convert('RGB')\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "input_tensor = transform(image).unsqueeze(0).to(device)  # 배치 차원 추가\n",
        "\n",
        "# 예측\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "\n",
        "print(\"예측된 양 등급:\", q_labels[predicted.item()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8R_9wIopHN1",
        "outputId": "ac759096-7a57-427d-a3cc-97e2393b8614"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측된 양 등급: Q1\n"
          ]
        }
      ]
    }
  ]
}